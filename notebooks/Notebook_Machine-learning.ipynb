{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook goal: Setup a basic machine learning framework that cleans data, standardizes features,\n",
    "#  evaluates feature impt, shap values, and a myriad of ML algorithms\n",
    "# TODO: add the day-of-week as a feature\n",
    "# TODO: Add in target date versus historic reference dates\n",
    "# TODO: Add in volume-based feature functionality\n",
    "# TODO: Evaluate standardizing features per stock or one model per stock - may not be enough data realistically\n",
    "# TODO: Check bol-range-pct calculation - only giving zero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions:\n",
    "\n",
    "def clean_stock_data(dataframe: pd.DataFrame) -> pd.DataFrame :\n",
    "\n",
    "    '''removes nulls and in the future will be built out to do any additonal cleaning on the dataframe that is necessary\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing all of the potential features\n",
    "        parameters: \n",
    "            calculation_field: field on which all of the features are built\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataset that is ready to load into a machine learning framework\n",
    "    '''\n",
    "\n",
    "    #TODO: In pipeline write this output to the \n",
    "    # remove records the preceed the target period to have complete information:\n",
    "    dataframe.dropna(inplace = True)\n",
    "    #dataframe = dataframe.reset_index(drop = True) # we won't reset the index for now for traceability back to the date, ticker combination later after training\n",
    "\n",
    "    # set the date as an index to us post-forecasting: This is a bad idea, come back to the concept\n",
    "    #dataframe.set_index(keys = 'date', verify_integrity = False, inplace = True) # verify integrity Fale to allow duplicates**\n",
    "    \n",
    "    # remove fields that will not be used as predictive features (can be hardcoded since dataframe structure will be the same):\n",
    "    dataframe = dataframe.drop(columns = [ 'date', 'high', 'low', 'open', 'volume', 'adj_close'])\n",
    "    \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def identify_fields_to_standardize(dataframe: pd.DataFrame, parameters: Dict) -> np.array :\n",
    "\n",
    "    '''creates a list of the continuous fields to standardize by dimension within the predictive model; NOTE: this is used within the standardizer\n",
    "    \n",
    "    Args:\n",
    "        dataframe: dataframe that contains all of the fields of interest to be used in the calculations\n",
    "        parameters:\n",
    "            continuous_feature_cutoff: ratio of unique values to record count to be used to codify continuous features -> removes records from the standardization process which don't have enough data to standardize (e.g., boolean)\n",
    "\n",
    "    Returns: list of continuous fields to use in the standardization process based on user's specifications of \"uniqueness\" threshold    \n",
    "\n",
    "    '''\n",
    "\n",
    "    numeric_fields = dataframe.select_dtypes(include = 'number').columns\n",
    "    records = len(dataframe)\n",
    "\n",
    "    record_summary = pd.DataFrame(dataframe[numeric_fields].nunique(), columns = ['unique_values'])\n",
    "    record_summary['rows_in_df'] = records\n",
    "    record_summary['value_to_record_ratio'] = record_summary['unique_values']/ record_summary['rows_in_df']\n",
    "\n",
    "    # filter for a threshold specified by the user:\n",
    "    record_summary = record_summary[record_summary['value_to_record_ratio'] > parameters['continuous_feature_cutoff']]\n",
    "\n",
    "    # remove percentage features # TODO: later add in functionality to remove percentage based features\n",
    "\n",
    "    return record_summary.index\n",
    "\n",
    "\n",
    "# Justification for approach on scaling - the argument can be made that since our approach will generalize movemements across multiple securities that we need to standardize each security to its own price range.  Therefore, any features with price-relative values will be scaled per the security's price values to avoid odd splits in tree-based algos\n",
    "# the concern with standardization is generally focused on not letting any one feature have considerably more weight in a model than another; however in this case, \n",
    "\n",
    "\n",
    "def standardize_continuous_features(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''function that identifies the continuious features in the dataframe and standardizes each feature by equity to enable scaling relative to each equity\n",
    "    \n",
    "    Args:\n",
    "        Dataframe: Pandas dataframe to be used in machine learning\n",
    "        Parameters:\n",
    "            stock_field: field indicating the stock for the window function to scan\n",
    "            calculation_field: field for which the target is being calculated (used for drop in main row merge)\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe: containing the standardized data fields\n",
    "    \n",
    "    '''\n",
    "\n",
    "    continuous_fields = list(identify_fields_to_standardize(dataframe = dataframe, parameters = parameters))\n",
    "\n",
    "    # add in the ticker for grouping next:\n",
    "    continuous_fields.append(parameters['stock_field'])\n",
    "\n",
    "    # downselect to the fields that will be used to standardize:\n",
    "    continuous_dataframe = dataframe[continuous_fields]\n",
    "\n",
    "    # calculate z-scores: --> Standardizes within each feature to scale accordingly\n",
    "    z_scores = (continuous_dataframe - continuous_dataframe.groupby(by = parameters['stock_field']).transform('mean')) / continuous_dataframe.groupby(by = parameters['stock_field']).transform('std')\n",
    "\n",
    "    # drop the null ticker (not needed post groupby): \n",
    "    z_scores.drop(columns = [ parameters['stock_field'], parameters['calculation_field'] ], inplace = True)\n",
    "\n",
    "    # rename the fields to indicate standardization:\n",
    "    z_scores.columns = z_scores.columns + '_std'\n",
    "\n",
    "    # drop original continuous fields # TODO: coming back after calculation checks:\n",
    "    if parameters['drop_original_fields'] == True:\n",
    "        continuous_fields.remove(parameters['stock_field'])\n",
    "        dataframe.drop(columns = continuous_fields, inplace = True)\n",
    "\n",
    "    # append the fields back into the core dataframe:\n",
    "    z_scores = pd.concat([dataframe, z_scores], axis = 1)\n",
    "\n",
    "    # remove the standardized target field:\n",
    "    z_scores.drop(columns = z_scores.columns[z_scores.columns.str.contains('target')][1], inplace = True)\n",
    "\n",
    "    # remove unnecessary items:\n",
    "    del continuous_fields, continuous_dataframe\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode_tickers(dataframe: pd.DataFrame, parameters: Dict) -> pd.DataFrame:\n",
    "\n",
    "    '''Returns one-hot encoded features to the predictive dataset NOTE: May not work, but this retains some of the information in the original dataframe while also potentially giving the global model a nudge\n",
    "       Note: we choose not to drop first for now, even though it's a trap; Can be used post processing or as model features\n",
    "    Args:\n",
    "        dataframe: core dataset that has been augmented with additional features\n",
    "        parameters:\n",
    "            stock_field: text field containing the \n",
    "    Returns:   \n",
    "        dataframe with augmented columns\n",
    "    \n",
    "    '''\n",
    "\n",
    "    dataframe = pd.get_dummies(data = dataframe, prefix = \"ind\", columns = [parameters['stock_field']], drop_first = False)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def create_training_test_splits(dataframe: pd.DataFrame, parameters: Dict) -> Tuple:\n",
    "\n",
    "    '''Function that splits out training and test sets for machine learning; for the purposes of this model the way we piose the problem allows for random train test split\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing only the target field and the features to be used by the classifier\n",
    "        parameters:\n",
    "            test_ratio: proportion of samples in the dataframe to be used as a test set once the models are tuned and evaluated\n",
    "\n",
    "    '''\n",
    "\n",
    "    # define Y and x:\n",
    "    target_feature = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    y = dataframe[target_feature]\n",
    "    X = dataframe.drop(columns = target_feature)\n",
    "\n",
    "    # create the training and test splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=parameters['test_size'], random_state=parameters['seed'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[06/11/23 11:21:30] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading data from <span style=\"color: #008000; text-decoration-color: #008000\">'combined_modeling_input'</span> <span style=\"font-weight: bold\">(</span>CSVDataSet<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>        <a href=\"file:///opt/anaconda3/envs/stock-classification/lib/python3.7/site-packages/kedro/io/data_catalog.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_catalog.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/stock-classification/lib/python3.7/site-packages/kedro/io/data_catalog.py#344\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">344</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[06/11/23 11:21:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading data from \u001b[32m'combined_modeling_input'\u001b[0m \u001b[1m(\u001b[0mCSVDataSet\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m        \u001b]8;id=44944;file:///opt/anaconda3/envs/stock-classification/lib/python3.7/site-packages/kedro/io/data_catalog.py\u001b\\\u001b[2mdata_catalog.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=792196;file:///opt/anaconda3/envs/stock-classification/lib/python3.7/site-packages/kedro/io/data_catalog.py#344\u001b\\\u001b[2m344\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = catalog.load('combined_modeling_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: clean stock data:\n",
    "\n",
    "df = clean_stock_data(dataframe = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'continuous_feature_cutoff' : 0.6,\n",
    "              'stock_field' : 'ticker',\n",
    "              'calculation_field' : 'close',\n",
    "              'drop_original_fields' : True,\n",
    "              'drop_stock_field': True, # keep this fixed \n",
    "              'train_size' : 0.20,\n",
    "              'seed' : 1187\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: standardize features:\n",
    "test = standardize_continuous_features(dataframe = df, parameters = parameters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode: \n",
    "test = one_hot_encode_tickers(dataframe = test, parameters= parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Function development HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = one_hot_encode_tickers(dataframe= test, parameters = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>above_7_close_sma_ind</th>\n",
       "      <th>above_14_close_sma_ind</th>\n",
       "      <th>above_21_close_sma_ind</th>\n",
       "      <th>cum_days_above_above_7_close_sma_ind</th>\n",
       "      <th>cum_days_above_above_14_close_sma_ind</th>\n",
       "      <th>cum_days_above_above_21_close_sma_ind</th>\n",
       "      <th>bol_range_pct</th>\n",
       "      <th>target_20_days_ahead_ind</th>\n",
       "      <th>14_close_sma_std</th>\n",
       "      <th>14_close_sma_pct_diff_std</th>\n",
       "      <th>14_close_std_std</th>\n",
       "      <th>21_close_sma_std</th>\n",
       "      <th>21_close_sma_pct_diff_std</th>\n",
       "      <th>21_close_std_std</th>\n",
       "      <th>7_close_sma_std</th>\n",
       "      <th>7_close_sma_pct_diff_std</th>\n",
       "      <th>7_close_std_std</th>\n",
       "      <th>bol_pct_from_bottom_std</th>\n",
       "      <th>bol_pct_from_top_std</th>\n",
       "      <th>bol_range_std</th>\n",
       "      <th>lower_bollinger_band_std</th>\n",
       "      <th>upper_bollinger_band_std</th>\n",
       "      <th>ind_AAPL</th>\n",
       "      <th>ind_XLE</th>\n",
       "      <th>ind_XLF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.595435</td>\n",
       "      <td>-1.376604</td>\n",
       "      <td>-0.986506</td>\n",
       "      <td>-1.593107</td>\n",
       "      <td>-1.215422</td>\n",
       "      <td>-1.125114</td>\n",
       "      <td>-1.595039</td>\n",
       "      <td>-1.734311</td>\n",
       "      <td>-0.509108</td>\n",
       "      <td>1.030173</td>\n",
       "      <td>1.116424</td>\n",
       "      <td>-1.125114</td>\n",
       "      <td>-1.577246</td>\n",
       "      <td>-1.598994</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.589468</td>\n",
       "      <td>-1.232396</td>\n",
       "      <td>-0.897239</td>\n",
       "      <td>-1.590706</td>\n",
       "      <td>-1.174615</td>\n",
       "      <td>-1.051856</td>\n",
       "      <td>-1.584461</td>\n",
       "      <td>-1.340545</td>\n",
       "      <td>-0.435440</td>\n",
       "      <td>1.137107</td>\n",
       "      <td>0.938561</td>\n",
       "      <td>-1.051856</td>\n",
       "      <td>-1.582146</td>\n",
       "      <td>-1.590141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.580557</td>\n",
       "      <td>-1.670412</td>\n",
       "      <td>-0.782983</td>\n",
       "      <td>-1.582594</td>\n",
       "      <td>-1.530326</td>\n",
       "      <td>-1.009586</td>\n",
       "      <td>-1.568888</td>\n",
       "      <td>-1.761056</td>\n",
       "      <td>-0.374703</td>\n",
       "      <td>1.557244</td>\n",
       "      <td>1.241071</td>\n",
       "      <td>-1.009586</td>\n",
       "      <td>-1.577831</td>\n",
       "      <td>-1.578709</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.571705</td>\n",
       "      <td>-1.846688</td>\n",
       "      <td>-0.635644</td>\n",
       "      <td>-1.575358</td>\n",
       "      <td>-1.697122</td>\n",
       "      <td>-0.900143</td>\n",
       "      <td>-1.555102</td>\n",
       "      <td>-1.856323</td>\n",
       "      <td>-0.221543</td>\n",
       "      <td>1.943526</td>\n",
       "      <td>1.225300</td>\n",
       "      <td>-0.900143</td>\n",
       "      <td>-1.581276</td>\n",
       "      <td>-1.562054</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.563611</td>\n",
       "      <td>-1.652952</td>\n",
       "      <td>-0.530130</td>\n",
       "      <td>-1.568013</td>\n",
       "      <td>-1.557752</td>\n",
       "      <td>-0.829720</td>\n",
       "      <td>-1.540041</td>\n",
       "      <td>-1.309295</td>\n",
       "      <td>-0.291924</td>\n",
       "      <td>1.936549</td>\n",
       "      <td>0.962560</td>\n",
       "      <td>-0.829720</td>\n",
       "      <td>-1.580639</td>\n",
       "      <td>-1.548809</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### - Testing functions HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_test_splits(dataframe: pd.DataFrame, parameters: Dict) -> Tuple:\n",
    "\n",
    "    '''Function that splits out training and test sets for machine learning; for the purposes of this model the way we piose the problem allows for random train test split\n",
    "    Args:\n",
    "        dataframe: pandas dataframe containing only the target field and the features to be used by the classifier\n",
    "        parameters:\n",
    "            test_ratio: proportion of samples in the dataframe to be used as a test set once the models are tuned and evaluated\n",
    "\n",
    "    '''\n",
    "\n",
    "    # define Y and x:\n",
    "    target_feature = list(dataframe.columns[dataframe.columns.str.contains('target')])\n",
    "\n",
    "    y = dataframe[target_feature]\n",
    "    X = dataframe.drop(columns = target_feature)\n",
    "\n",
    "    # create the training and test splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=parameters['test_size'], random_state=parameters['seed'])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'target_20_days_ahead_ind'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'target_20_days_ahead_ind'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_training_test_splits(dataframe = test, parameters  = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Index</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ticker'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'above_7_close_sma_ind'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'above_14_close_sma_ind'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'above_21_close_sma_ind'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cum_days_above_above_7_close_sma_ind'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'cum_days_above_above_14_close_sma_ind'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'cum_days_above_above_21_close_sma_ind'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bol_range_pct'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'target_20_days_ahead_ind'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'14_close_sma_std'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'14_close_sma_pct_diff_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'14_close_std_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'21_close_sma_std'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'21_close_sma_pct_diff_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'21_close_std_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'7_close_sma_std'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'7_close_sma_pct_diff_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'7_close_std_std'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'bol_pct_from_bottom_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bol_pct_from_top_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bol_range_std'</span>,\n",
       "       <span style=\"color: #008000; text-decoration-color: #008000\">'lower_bollinger_band_std'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'upper_bollinger_band_std'</span><span style=\"font-weight: bold\">]</span>,\n",
       "      <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35mIndex\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'date'\u001b[0m, \u001b[32m'ticker'\u001b[0m, \u001b[32m'above_7_close_sma_ind'\u001b[0m, \u001b[32m'above_14_close_sma_ind'\u001b[0m,\n",
       "       \u001b[32m'above_21_close_sma_ind'\u001b[0m, \u001b[32m'cum_days_above_above_7_close_sma_ind'\u001b[0m,\n",
       "       \u001b[32m'cum_days_above_above_14_close_sma_ind'\u001b[0m,\n",
       "       \u001b[32m'cum_days_above_above_21_close_sma_ind'\u001b[0m, \u001b[32m'bol_range_pct'\u001b[0m,\n",
       "       \u001b[32m'target_20_days_ahead_ind'\u001b[0m, \u001b[32m'14_close_sma_std'\u001b[0m,\n",
       "       \u001b[32m'14_close_sma_pct_diff_std'\u001b[0m, \u001b[32m'14_close_std_std'\u001b[0m, \u001b[32m'21_close_sma_std'\u001b[0m,\n",
       "       \u001b[32m'21_close_sma_pct_diff_std'\u001b[0m, \u001b[32m'21_close_std_std'\u001b[0m, \u001b[32m'7_close_sma_std'\u001b[0m,\n",
       "       \u001b[32m'7_close_sma_pct_diff_std'\u001b[0m, \u001b[32m'7_close_std_std'\u001b[0m,\n",
       "       \u001b[32m'bol_pct_from_bottom_std'\u001b[0m, \u001b[32m'bol_pct_from_top_std'\u001b[0m, \u001b[32m'bol_range_std'\u001b[0m,\n",
       "       \u001b[32m'lower_bollinger_band_std'\u001b[0m, \u001b[32m'upper_bollinger_band_std'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "      \u001b[33mdtype\u001b[0m=\u001b[32m'object'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classifiers to use: support vactor machine, decision tree, random forest, xgboost, adaboost\n",
    "\n",
    "def train_models(X_train: pd.DataFrame, y_train: pd.Series, parameters) -> pd.DataFrame:\n",
    "\n",
    "    '''Trains a series of machine learning model outputs for evaluation by the user\n",
    "    \n",
    "    Args:\n",
    "        X_train: inputs from train-test split function\n",
    "        y_train: y-series from the train-test split function\n",
    "\n",
    "    Returns:\n",
    "        Summarized output of all ML models tried\n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10939df48fcda05c4bec3e401945f701fe0f8ffca207b5e1a18717f88993ca17"
  },
  "kernelspec": {
   "display_name": "Kedro (stock_price_classification)",
   "language": "python",
   "name": "kedro_stock_price_classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
